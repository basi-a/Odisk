# Settings and configurations that are common for all containers
x-common: &common
  extra_hosts:
      - "db:172.40.20.10"
      - "adminer:172.40.20.11"
      - "redis:172.40.20.12"
      - "nginx:172.40.20.13"
      - "keepalived.haproxy1:172.40.20.14"
      - "keepalived.haproxy2:172.40.20.15"
      - "minio1:172.40.20.16"
      - "minio2:172.40.20.17"
      - "minio3:172.40.20.18"
      - "minio4:172.40.20.19"
      - "server1:172.40.20.20"
      - "server2:172.40.20.21"
      - "server3:172.40.20.22"
      - "server4:172.40.20.23"
      - "nsqlookupd:172.40.20.24"
      - "nsqd:172.40.20.25"
      - "nsqadmin:172.40.20.26"
      - "keepalived.vip:172.40.20.100"
  volumes: 
    - /etc/localtime:/etc/localtime:ro
  restart: always
x-minio-common: &minio-common
  <<: *common
  build:
    context: ./minio/.
  image: odisk/minio:latest
  expose:
    - "9000"
    - "9001"
  command: server --console-address ":9001" https://minio{1...4}/data{1...2} 
  environment:
    MINIO_ROOT_USER: minioadmin
    MINIO_ROOT_PASSWORD: minioadmin
    MINIO_REGION_NAME: zh-private-1
    MINIO_SERVER_URL: https://172.40.20.100:9000
    MINIO_BROWSER_REDIRECT_URL: https://172.40.20.100:9000
    MC_HOST_local: https://minioadmin:minioadmin@localhost:9000

  healthcheck:
    test: ["CMD", "mc", "ready", "local", "--insecure"]
    interval: 10s
    retries: 10
    timeout: 5s

x-keepalived-haproxy-common: &keepalived-haproxy-common
  <<: *common
  build:
    context: ./haproxy-keepalived/.
  image: odisk/keepalived-haproxy:latest
  healthcheck:
    test: ["CMD", "curl", "-u" ,"admin:admin","-I","http://172.40.20.100:8080/admin"]
    interval: 10s
    retries: 10
    timeout: 5s
  depends_on: 
    minio1:
      condition: service_healthy
    minio2:
      condition: service_healthy
    minio3:
      condition: service_healthy
    minio4:
      condition: service_healthy

  command: /sbin/init

  privileged: true  

x-server-common: &server-common
  <<: *common
  build:
    context: ./server/.
  image: odisk/server:latest
  volumes:
    - ./server/config/config.yaml:/etc/odisk/config.yaml
  expose: 
    - "7000"
  command: /opt/odisk
  healthcheck:
    test: ["CMD", "curl","-I", "-k", "https://localhost:7000/ping"]
    interval: 5m
    retries: 10
    timeout: 5s
  depends_on:
    minio1:
      condition: service_healthy
    minio2:
      condition: service_healthy
    minio3:
      condition: service_healthy
    minio4:
      condition: service_healthy
    db:
      condition: service_healthy
    redis:
      condition: service_healthy
    nsqd:
      condition: service_healthy


services:
  # db:
  #   <<: *common
  #   image: mariadb:latest
  #   hostname: mariadb
  #   container_name: odisk-mariadb
  #   restart: always
  #   ports:
  #     - "13306:3306"
  #   volumes:
  #     - ./db/data:/var/lib/mysql
  #   environment:
  #     MARIADB_MYSQL_LOCALHOST_USER: 1
  #     MYSQL_ROOT_PASSWORD: 'FTVGBUY7uvib89Y'
  #     MYSQL_DATABASE: 'odisk'
  #     MYSQL_USER: 'odisk'
  #     MYSQL_PASSWORD: 'FTVGBUY7uvib89Y'
  #     MYSQL_CHARSET: 'utf8mb4'
  #     MYSQL_COLLATION: 'utf8mb4_general_ci'
  #     MYSQL_HOST: '%'
  #   healthcheck:
  #     test: ["CMD", "/usr/local/bin/healthcheck.sh", "--connect"]
  #     interval: 5s
  #     retries: 5
  #     timeout: 5s
  #   networks: 
  #     app-network:
  #       ipv4_address: 172.40.20.10
  db:
    <<: *common
    image: postgres:16.2-alpine
    hostname: postgres
    container_name: odisk-postgres
   
    ports:
      - "15432:5432"
    # set shared memory limit when using docker-compose
    shm_size: 128mb

    environment:
      TZ: "Asia/Shanghai"
      PGTZ: "Asia/Shanghai"
      POSTGRES_USER: "odisk"
      POSTGRES_PASSWORD: "FTVGBUY7uvib89Y"
      POSTGRES_DB: "odisk"
      PGDATA: "/var/lib/postgresql/data/pgdata"
      POSTGRES_HOST_AUTH_METHOD: "scram-sha-256"
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256"
    volumes:
      - ./db/data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U","odisk"]
      interval: 10s
      timeout: 3s
      retries: 3
    networks: 
      app-network:
        ipv4_address: 172.40.20.10

  adminer:
    <<: *common
    image: adminer:latest
    hostname: adminer
    container_name: odisk-adminer

    ports:
      - "28080:8080"
    depends_on:
      db:
        condition: service_healthy
    networks: 
      app-network:
        ipv4_address: 172.40.20.11

  redis:
    <<: *common
    image: redis:latest
    hostname: redis
    container_name: odisk-redis

    command: redis-server --requirepass TGVG8791HUBH --appendonly yes --bind 0.0.0.0
    ports:  
      - "16379:6379"
    healthcheck:
      test: ["CMD", "redis-cli","ping"]
      interval: 10s
      retries: 10
      timeout: 5s
    networks: 
      app-network:
        ipv4_address: 172.40.20.12

  nginx:
    <<: *common
    image: nginx
    hostname: nginx
    container_name: odisk-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/odisk-font.conf:/etc/nginx/conf.d/odisk-font.conf
      - ./nginx/dist:/opt/dist
      - ./nginx/cert:/opt/cert

    command: ["/bin/bash", "-c", "chown -R nginx:nginx /opt/dist /opt/cert && nginx -g 'daemon off;'"]

    depends_on:
      - db 
      - redis
      - keepalived_haproxy1
      - keepalived_haproxy2
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost"]
      interval: 30s
      retries: 10
      timeout: 5s
    networks: 
      app-network:
        ipv4_address: 172.40.20.13

  keepalived_haproxy1:
    <<: *keepalived-haproxy-common
    container_name: odisk-keepalived_haproxy1
    volumes:
      - ./haproxy-keepalived/config/keepalived1.conf:/etc/keepalived/keepalived.conf
      - ./haproxy-keepalived/config/haproxy.cfg:/etc/haproxy/haproxy.cfg
    networks: 
      app-network:
        ipv4_address: 172.40.20.14

  keepalived_haproxy2:
    <<: *keepalived-haproxy-common
    container_name: odisk-keepalived_haproxy2
    volumes:
      - ./haproxy-keepalived/config/keepalived2.conf:/etc/keepalived/keepalived.conf
      - ./haproxy-keepalived/config/haproxy.cfg:/etc/haproxy/haproxy.cfg
    networks: 
      app-network:
        ipv4_address: 172.40.20.15

  minio1:
    <<: *minio-common
    hostname: minio1
    container_name: odisk-minio1
    volumes:
      - ./minio/data/data1-1:/data1
      - ./minio/data/data1-2:/data2
    networks: 
      app-network:
        ipv4_address: 172.40.20.16
  minio2:
    <<: *minio-common
    hostname: minio2
    container_name: odisk-minio2
    volumes:
      - ./minio/data/data2-1:/data1
      - ./minio/data/data2-2:/data2
    networks: 
      app-network:
        ipv4_address: 172.40.20.17
  minio3:
    <<: *minio-common
    hostname: minio3
    container_name: odisk-minio3
    volumes:
      - ./minio/data/data3-1:/data1
      - ./minio/data/data3-2:/data2
    networks: 
      app-network:
        ipv4_address: 172.40.20.18
  minio4:
    <<: *minio-common
    hostname: minio4
    container_name: odisk-minio4
    volumes:
      - ./minio/data/data4-1:/data1
      - ./minio/data/data4-2:/data2
    networks: 
      app-network:
        ipv4_address: 172.40.20.19

  server1:
    <<: *server-common
    hostname: server1
    container_name: odisk-server1
    networks: 
      app-network:
        ipv4_address: 172.40.20.20
  server2:
    <<: *server-common
    hostname: server2
    container_name: odisk-server2
    networks: 
      app-network:
        ipv4_address: 172.40.20.21
  server3:
    <<: *server-common
    hostname: server3
    container_name: odisk-server3
    networks: 
      app-network:
        ipv4_address: 172.40.20.22
  server4:
    <<: *server-common
    hostname: server4
    container_name: odisk-server4
    networks: 
      app-network:
        ipv4_address: 172.40.20.23

  nsqlookupd:
    <<: *common
    image: nsqio/nsq
    container_name: odisk-nsqlookupd
    command: /nsqlookupd --broadcast-address=172.40.20.24
    ports:
      - "4160"
      - "4161"
    healthcheck:
      test: ["CMD", "pgrep", "-f", "nsqlookupd"]
      interval: 30s
      timeout: 30s
      retries: 5
    networks: 
      app-network:
        ipv4_address: 172.40.20.24
  nsqd:
    <<: *common
    image: nsqio/nsq
    container_name: odisk-nsqd
    command: /nsqd --lookupd-tcp-address=172.40.20.24:4160 --broadcast-address=172.40.20.25
    depends_on:
      - nsqlookupd
    ports:
      - "4150"
      - "4151"
    healthcheck:
      test: ["CMD", "pgrep", "-f", "nsqd"]
      interval: 30s
      timeout: 30s
      retries: 5
    networks: 
      app-network:
        ipv4_address: 172.40.20.25
  nsqadmin:
    <<: *common
    image: nsqio/nsq
    container_name: odisk-nsqadmin
    command: /nsqadmin --lookupd-http-address=172.40.20.24:4161
    depends_on:
      - nsqlookupd  
    ports:
      - "4171:4171"
    networks: 
      app-network:
        ipv4_address: 172.40.20.26

networks:
  app-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.40.20.0/24
          gateway: 172.40.20.1
